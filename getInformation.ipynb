{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10f056d-0e34-4e0c-b75f-44815160ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6ae122-5bd6-43ce-bd24-7b343c87bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç›®æ ‡ç½‘å€\n",
    "base_url = \"https://www.deanza.edu/hefas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c632b7ea-fb1d-496f-961a-b0246a88557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå­˜å‚¨ç›®å½•\n",
    "output_dir = \"HEFAS_Knowledge\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d04d1705-5c9e-43c5-ae46-b04b34a15357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\" è·å–ç½‘é¡µçš„ BeautifulSoup å¯¹è±¡ \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed to fetch {url} (Status: {response.status_code})\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ \"\"\"\n",
    "    return \" \".join(text.split()).strip()\n",
    "\n",
    "def extract_main_content(soup, url):\n",
    "    \"\"\" æå–ç½‘é¡µä¸»è¦å†…å®¹ï¼Œå¹¶å¤„ç†æ ¼å¼ \"\"\"\n",
    "    title = clean_text(soup.title.text) if soup.title else \"Untitled\"\n",
    "\n",
    "    # æå–æ‰€æœ‰æ®µè½ã€æ ‡é¢˜ã€åˆ—è¡¨\n",
    "    content = []\n",
    "    \n",
    "    # æå–ä¸»æ ‡é¢˜\n",
    "    content.append(f\"# {title}\\n\")\n",
    "\n",
    "    # æå–æ‰€æœ‰å­æ ‡é¢˜ <h1> - <h6>\n",
    "    for heading in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        level = heading.name[1]  # è·å– h1-h6 çš„çº§åˆ«\n",
    "        content.append(f\"{'#' * int(level)} {clean_text(heading.text)}\\n\")\n",
    "\n",
    "    # æå–æ®µè½ <p>\n",
    "    for paragraph in soup.find_all(\"p\"):\n",
    "        text = clean_text(paragraph.text)\n",
    "        if text:\n",
    "            content.append(text + \"\\n\")\n",
    "\n",
    "    # æå–åˆ—è¡¨ <ul> <li>\n",
    "    for ul in soup.find_all(\"ul\"):\n",
    "        for li in ul.find_all(\"li\"):\n",
    "            text = clean_text(li.text)\n",
    "            if text:\n",
    "                content.append(f\"- {text}\")\n",
    "\n",
    "    # æå–é“¾æ¥ <a>\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        link_text = clean_text(a.text) or \"Link\"\n",
    "        href = a[\"href\"]\n",
    "        if not href.startswith(\"http\"):  # å¤„ç†ç›¸å¯¹è·¯å¾„\n",
    "            href = \"https://www.deanza.edu\" + href\n",
    "        links.append(f\"- [{link_text}]({href})\")\n",
    "\n",
    "    if links:\n",
    "        content.append(\"\\n## Related Links\\n\" + \"\\n\".join(links))\n",
    "\n",
    "    return title, \"\\n\".join(content)\n",
    "\n",
    "def save_markdown(title, content):\n",
    "    \"\"\" ä¿å­˜å†…å®¹ä¸º Markdown æ ¼å¼ \"\"\"\n",
    "    filename = f\"{output_dir}/{title.replace(' ', '_').replace('/', '-')}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"âœ… Saved: {filename}\")\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\" çˆ¬å–å•ä¸ªç½‘é¡µå¹¶å­˜ä¸º Markdown \"\"\"\n",
    "    soup = get_soup(url)\n",
    "    if soup:\n",
    "        title, content = extract_main_content(soup, url)\n",
    "        save_markdown(title, content)\n",
    "\n",
    "def scrape_all_pages():\n",
    "    \"\"\" çˆ¬å– HEFAS ä¸»é¡µé¢ + å…¶æ‰€æœ‰å­é¡µé¢ \"\"\"\n",
    "    soup = get_soup(base_url)\n",
    "    if not soup:\n",
    "        return\n",
    "\n",
    "    # çˆ¬å–ä¸»é¡µé¢\n",
    "    scrape_page(base_url)\n",
    "\n",
    "    # è·å–æ‰€æœ‰å­é“¾æ¥ï¼ˆåŒåŸŸåä¸‹çš„é¡µé¢ï¼‰\n",
    "    sub_links = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/hefas/\") and not href.startswith(\"http\"):\n",
    "            full_url = \"https://www.deanza.edu\" + href\n",
    "            sub_links.add(full_url)\n",
    "\n",
    "    print(f\"ğŸ” Found {len(sub_links)} sub-pages\")\n",
    "\n",
    "    # çˆ¬å–æ‰€æœ‰å­é¡µé¢\n",
    "    for link in sub_links:\n",
    "        scrape_page(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d988242-223f-40fb-864f-51f21a5ecc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: HEFAS_Knowledge/HEFAS.md\n",
      "ğŸ” Found 15 sub-pages\n",
      "âœ… Saved: HEFAS_Knowledge/Important_Legislation.md\n",
      "âœ… Saved: HEFAS_Knowledge/Untitled.md\n",
      "âœ… Saved: HEFAS_Knowledge/Undocumented_Student_Week_of_Action.md\n",
      "âœ… Saved: HEFAS_Knowledge/Volunteering.md\n",
      "âœ… Saved: HEFAS_Knowledge/Resources.md\n",
      "âœ… Saved: HEFAS_Knowledge/HEFAS_Interns.md\n",
      "âœ… Saved: HEFAS_Knowledge/Members.md\n",
      "âœ… Saved: HEFAS_Knowledge/Internships.md\n",
      "âœ… Saved: HEFAS_Knowledge/HEFAS.md\n",
      "âœ… Saved: HEFAS_Knowledge/UndocuSTEM_Program.md\n",
      "âœ… Saved: HEFAS_Knowledge/UndocuSol.md\n",
      "âœ… Saved: HEFAS_Knowledge/Donations.md\n",
      "âœ… Saved: HEFAS_Knowledge/Legal_Services.md\n",
      "âœ… Saved: HEFAS_Knowledge/HEFAS_Annual_Summit.md\n",
      "âœ… Saved: HEFAS_Knowledge/Meet_With_Us.md\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œçˆ¬å–\n",
    "scrape_all_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac47428-9597-40cd-b9aa-6fb60bd3f30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
