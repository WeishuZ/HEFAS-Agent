{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10f056d-0e34-4e0c-b75f-44815160ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f6ae122-5bd6-43ce-bd24-7b343c87bada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 目标网址\n",
    "base_url = \"https://www.deanza.edu/hefas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c632b7ea-fb1d-496f-961a-b0246a88557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建存储目录\n",
    "output_dir = \"HEFAS_Knowledge\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d04d1705-5c9e-43c5-ae46-b04b34a15357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(url):\n",
    "    \"\"\" 获取网页的 BeautifulSoup 对象 \"\"\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.text, \"html.parser\")\n",
    "    else:\n",
    "        print(f\"❌ Failed to fetch {url} (Status: {response.status_code})\")\n",
    "        return None\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\" 清理文本，去除多余空格和换行 \"\"\"\n",
    "    return \" \".join(text.split()).strip()\n",
    "\n",
    "def extract_main_content(soup, url):\n",
    "    \"\"\" 提取网页主要内容，并处理格式 \"\"\"\n",
    "    title = clean_text(soup.title.text) if soup.title else \"Untitled\"\n",
    "\n",
    "    # 提取所有段落、标题、列表\n",
    "    content = []\n",
    "    \n",
    "    # 提取主标题\n",
    "    content.append(f\"# {title}\\n\")\n",
    "\n",
    "    # 提取所有子标题 <h1> - <h6>\n",
    "    for heading in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]):\n",
    "        level = heading.name[1]  # 获取 h1-h6 的级别\n",
    "        content.append(f\"{'#' * int(level)} {clean_text(heading.text)}\\n\")\n",
    "\n",
    "    # 提取段落 <p>\n",
    "    for paragraph in soup.find_all(\"p\"):\n",
    "        text = clean_text(paragraph.text)\n",
    "        if text:\n",
    "            content.append(text + \"\\n\")\n",
    "\n",
    "    # 提取列表 <ul> <li>\n",
    "    for ul in soup.find_all(\"ul\"):\n",
    "        for li in ul.find_all(\"li\"):\n",
    "            text = clean_text(li.text)\n",
    "            if text:\n",
    "                content.append(f\"- {text}\")\n",
    "\n",
    "    # 提取链接 <a>\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        link_text = clean_text(a.text) or \"Link\"\n",
    "        href = a[\"href\"]\n",
    "        if not href.startswith(\"http\"):  # 处理相对路径\n",
    "            href = \"https://www.deanza.edu\" + href\n",
    "        links.append(f\"- [{link_text}]({href})\")\n",
    "\n",
    "    if links:\n",
    "        content.append(\"\\n## Related Links\\n\" + \"\\n\".join(links))\n",
    "\n",
    "    return title, \"\\n\".join(content)\n",
    "\n",
    "def save_markdown(title, content):\n",
    "    \"\"\" 保存内容为 Markdown 格式 \"\"\"\n",
    "    filename = f\"{output_dir}/{title.replace(' ', '_').replace('/', '-')}.md\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    print(f\"✅ Saved: {filename}\")\n",
    "\n",
    "def scrape_page(url):\n",
    "    \"\"\" 爬取单个网页并存为 Markdown \"\"\"\n",
    "    soup = get_soup(url)\n",
    "    if soup:\n",
    "        title, content = extract_main_content(soup, url)\n",
    "        save_markdown(title, content)\n",
    "\n",
    "def scrape_all_pages():\n",
    "    \"\"\" 爬取 HEFAS 主页面 + 其所有子页面 \"\"\"\n",
    "    soup = get_soup(base_url)\n",
    "    if not soup:\n",
    "        return\n",
    "\n",
    "    # 爬取主页面\n",
    "    scrape_page(base_url)\n",
    "\n",
    "    # 获取所有子链接（同域名下的页面）\n",
    "    sub_links = set()\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/hefas/\") and not href.startswith(\"http\"):\n",
    "            full_url = \"https://www.deanza.edu\" + href\n",
    "            sub_links.add(full_url)\n",
    "\n",
    "    print(f\"🔍 Found {len(sub_links)} sub-pages\")\n",
    "\n",
    "    # 爬取所有子页面\n",
    "    for link in sub_links:\n",
    "        scrape_page(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d988242-223f-40fb-864f-51f21a5ecc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: HEFAS_Knowledge/HEFAS.md\n",
      "🔍 Found 15 sub-pages\n",
      "✅ Saved: HEFAS_Knowledge/Important_Legislation.md\n",
      "✅ Saved: HEFAS_Knowledge/Untitled.md\n",
      "✅ Saved: HEFAS_Knowledge/Undocumented_Student_Week_of_Action.md\n",
      "✅ Saved: HEFAS_Knowledge/Volunteering.md\n",
      "✅ Saved: HEFAS_Knowledge/Resources.md\n",
      "✅ Saved: HEFAS_Knowledge/HEFAS_Interns.md\n",
      "✅ Saved: HEFAS_Knowledge/Members.md\n",
      "✅ Saved: HEFAS_Knowledge/Internships.md\n",
      "✅ Saved: HEFAS_Knowledge/HEFAS.md\n",
      "✅ Saved: HEFAS_Knowledge/UndocuSTEM_Program.md\n",
      "✅ Saved: HEFAS_Knowledge/UndocuSol.md\n",
      "✅ Saved: HEFAS_Knowledge/Donations.md\n",
      "✅ Saved: HEFAS_Knowledge/Legal_Services.md\n",
      "✅ Saved: HEFAS_Knowledge/HEFAS_Annual_Summit.md\n",
      "✅ Saved: HEFAS_Knowledge/Meet_With_Us.md\n"
     ]
    }
   ],
   "source": [
    "# 运行爬取\n",
    "scrape_all_pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac47428-9597-40cd-b9aa-6fb60bd3f30b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
